import torch
import os
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel
from huggingface_hub import HfApi, ModelCard, ModelCardData
from app.config import CACHE_DIR, SAVE_DIR, HF_TOKEN, HF_USERNAME

# ---------------------------------------------------------
# AYARLAR
# ---------------------------------------------------------
BASE_MODEL_ID = "Qwen/Qwen3-0.6B"
DATASET_ID = "turkerberkdonmez/TUSGPT-TR-Medical-Dataset-v1"

# Yeni modelin adÄ±
NEW_MODEL_NAME = "Qwen3-0.6B-Medical-SFT"

REPO_ID = f"{HF_USERNAME}/{NEW_MODEL_NAME}"
# ---------------------------------------------------------

MODEL_CARD_TEMPLATE = """---
language:
  - tr
license: apache-2.0
library_name: transformers
tags:
  - medical
  - turkish
  - sft
  - fine-tuned
  - qwen3
  - lora
base_model: {base_model}
datasets:
  - {dataset}
pipeline_tag: text-generation
---

# ğŸ¥ {model_name}

**{model_name}**, TÃ¼rkÃ§e tÄ±bbi sorulara doÄŸru ve kapsamlÄ± yanÄ±tlar Ã¼retmek amacÄ±yla fine-tune edilmiÅŸ bir dil modelidir.

## ğŸ“‹ Model DetaylarÄ±

| Ã–zellik | DeÄŸer |
|---|---|
| **Base Model** | [{base_model}](https://huggingface.co/{base_model}) |
| **YÃ¶ntem** | SFT (Supervised Fine-Tuning) + LoRA |
| **Dil** | TÃ¼rkÃ§e ğŸ‡¹ğŸ‡· |
| **Veri Seti** | [{dataset}](https://huggingface.co/datasets/{dataset}) |
| **Lisans** | Apache 2.0 |

## ğŸ§¬ EÄŸitim Bilgileri

- **LoRA Rank (r):** 16
- **LoRA Alpha:** 32
- **LoRA Dropout:** 0.05
- **Target Modules:** q_proj, k_proj, v_proj, o_proj, gate_proj, up_proj, down_proj
- **Precision:** bfloat16
- **Optimizer:** AdamW

## ğŸ’¡ KullanÄ±m

```python
from transformers import AutoTokenizer, AutoModelForCausalLM

model_id = "{repo_id}"
tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype="auto", device_map="auto")

messages = [
    {{"role": "system", "content": "Sen tÄ±p alanÄ±nda uzmanlaÅŸmÄ±ÅŸ, TÃ¼rkÃ§e yanÄ±t veren bir yapay zeka asistanÄ±sÄ±n."}},
    {{"role": "user", "content": "Hamilelikte baÅŸ aÄŸrÄ±sÄ± iÃ§in hangi ilaÃ§lar gÃ¼venlidir?"}}
]

text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
inputs = tokenizer([text], return_tensors="pt").to(model.device)

outputs = model.generate(**inputs, max_new_tokens=256, temperature=0.7, top_p=0.8, top_k=20)
response = tokenizer.decode(outputs[0][len(inputs.input_ids[0]):], skip_special_tokens=True)
print(response)
```

## âš ï¸ Sorumluluk Reddi

Bu model yalnÄ±zca araÅŸtÄ±rma ve eÄŸitim amaÃ§lÄ±dÄ±r. **TÄ±bbi teÅŸhis veya tedavi iÃ§in kullanÄ±lmamalÄ±dÄ±r.** SaÄŸlÄ±k sorunlarÄ±nÄ±z iÃ§in mutlaka bir saÄŸlÄ±k profesyoneline danÄ±ÅŸÄ±n.
"""


def create_model_card():
    """Profesyonel model kartÄ± oluÅŸtur"""
    return MODEL_CARD_TEMPLATE.format(
        base_model=BASE_MODEL_ID,
        dataset=DATASET_ID,
        model_name=NEW_MODEL_NAME,
        repo_id=REPO_ID,
    )


def main():
    if not HF_TOKEN:
        print("âŒ HATA: HF_TOKEN bulunamadÄ±! LÃ¼tfen .env dosyanÄ±zÄ± kontrol edin.")
        return

    print(f"ğŸ”„ Base model yÃ¼kleniyor: {BASE_MODEL_ID}")
    try:
        base_model = AutoModelForCausalLM.from_pretrained(
            BASE_MODEL_ID,
            torch_dtype=torch.float16,
            device_map="auto",
            trust_remote_code=False,
            cache_dir=CACHE_DIR
        )
        
        tokenizer = AutoTokenizer.from_pretrained(
            BASE_MODEL_ID, 
            trust_remote_code=False,
            cache_dir=CACHE_DIR
        )
    except Exception as e:
        print(f"âŒ Model yÃ¼kleme hatasÄ±: {e}")
        return

    adapter_path = f"{SAVE_DIR}{BASE_MODEL_ID}/"
    print(f"ğŸ”„ Adapter yÃ¼kleniyor: {adapter_path}")
    try:
        model = PeftModel.from_pretrained(base_model, adapter_path)
    except Exception as e:
        print(f"âŒ Adapter yÃ¼kleme hatasÄ±: {e}")
        return
    
    print("ğŸ”„ Model birleÅŸtiriliyor (Merge & Unload)...")
    model = model.merge_and_unload()

    print(f"ğŸš€ Hugging Face Hub'a yÃ¼kleniyor: {REPO_ID}")
    try:
        # Modeli yÃ¼kle
        model.push_to_hub(REPO_ID, token=HF_TOKEN)
        # Tokenizer'Ä± yÃ¼kle
        tokenizer.push_to_hub(REPO_ID, token=HF_TOKEN)

        # Model kartÄ±nÄ± yÃ¼kle
        print("ğŸ“ Model kartÄ± oluÅŸturuluyor...")
        api = HfApi(token=HF_TOKEN)
        model_card_content = create_model_card()
        api.upload_file(
            path_or_fileobj=model_card_content.encode("utf-8"),
            path_in_repo="README.md",
            repo_id=REPO_ID,
            repo_type="model",
        )
        
        print(f"\nâœ… Ä°ÅLEM BAÅARILI!")
        print(f"ğŸ”— Model Linki: https://huggingface.co/{REPO_ID}")
    except Exception as e:
        print(f"âŒ YÃ¼kleme sÄ±rasÄ±nda hata oluÅŸtu: {e}")

if __name__ == "__main__":
    main()